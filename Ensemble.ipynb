{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-20T09:56:21.494005500Z",
     "start_time": "2023-06-20T09:56:20.652564600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Load data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T09:56:21.667013Z",
     "start_time": "2023-06-20T09:56:21.495005300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#Extract features and labels\n",
    "X = train_data.drop(columns=['id', 'label'])\n",
    "y = train_data['label']\n",
    "test_X = test_data.drop(columns=['id'])\n",
    "training_dataset = train_data.drop(['id'],axis=1,inplace=False)\n",
    "test_dataset = test_data.drop(['id'],axis=1,inplace=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T09:56:21.683090400Z",
     "start_time": "2023-06-20T09:56:21.668012800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Classifier List\n",
    "classifiers = [\n",
    "     XGBClassifier(learning_rate=0.3,n_estimators=200,max_depth=7,min_child_weight=13,gamma=0,subsample=0.8,colsample_bytree=0.8,objective='binary:logistic',nthread=8,scale_pos_weight=1,eval_metric='auc'),\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=50, min_samples_split=10, min_samples_leaf=1,n_jobs=8)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T09:56:21.699090Z",
     "start_time": "2023-06-20T09:56:21.684088400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#mlp\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(MLPClassifier, self).__init__()\n",
    "            self.input = nn.Sequential(\n",
    "                nn.Embedding(input_size, hidden_size),\n",
    "            )\n",
    "            self.hidden = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "            self.output = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, output_size),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        data1 = self.hidden(self.input(data1))\n",
    "        data2 = self.hidden(self.input(data2))\n",
    "        data = torch.cat((data1, data2), dim=1)\n",
    "        label = self.output(data).squeeze()\n",
    "        return label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T09:56:21.713129600Z",
     "start_time": "2023-06-20T09:56:21.702052300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# pytorch data\n",
    "class NodeDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data1 = self.data.iloc[index]['id1']\n",
    "        data2 = self.data.iloc[index]['id2']\n",
    "        label = self.data.iloc[index]['label']\n",
    "        return data1,data2,label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T09:56:21.730208900Z",
     "start_time": "2023-06-20T09:56:21.715126500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Create a data loader\n",
    "batch_size = 2048\n",
    "train_dataloader = DataLoader(NodeDataset(training_dataset), batch_size=batch_size, shuffle=True)\n",
    "#Set network parameters\n",
    "input_size = training_dataset.shape[0]\n",
    "hidden_size = 128\n",
    "output_size = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T09:56:21.760287600Z",
     "start_time": "2023-06-20T09:56:21.731208800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T09:56:21.776543100Z",
     "start_time": "2023-06-20T09:56:21.760287600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# mlp\n",
    "mlp_clf = MLPClassifier(input_size, hidden_size, output_size).to(device)\n",
    "# loss and adam\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(mlp_clf.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T09:56:22.331379400Z",
     "start_time": "2023-06-20T09:56:21.777542800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [0/948232], Loss: 0.7590581179\n",
      "Epoch [1/10], Step [20480/948232], Loss: 0.4847901165\n",
      "Epoch [1/10], Step [40960/948232], Loss: 0.2758668661\n",
      "Epoch [1/10], Step [61440/948232], Loss: 0.1310569048\n",
      "Epoch [1/10], Step [81920/948232], Loss: 0.0678457245\n",
      "Epoch [1/10], Step [102400/948232], Loss: 0.0393403172\n",
      "Epoch [1/10], Step [122880/948232], Loss: 0.0301965084\n",
      "Epoch [1/10], Step [143360/948232], Loss: 0.0241983887\n",
      "Epoch [1/10], Step [163840/948232], Loss: 0.0173862744\n",
      "Epoch [1/10], Step [184320/948232], Loss: 0.0132012842\n",
      "Epoch [1/10], Step [204800/948232], Loss: 0.0120809069\n",
      "Epoch [1/10], Step [225280/948232], Loss: 0.0087572681\n",
      "Epoch [1/10], Step [245760/948232], Loss: 0.0091271959\n",
      "Epoch [1/10], Step [266240/948232], Loss: 0.0076054940\n",
      "Epoch [1/10], Step [286720/948232], Loss: 0.0059581553\n",
      "Epoch [1/10], Step [307200/948232], Loss: 0.0053987685\n",
      "Epoch [1/10], Step [327680/948232], Loss: 0.0053467527\n",
      "Epoch [1/10], Step [348160/948232], Loss: 0.0040545906\n",
      "Epoch [1/10], Step [368640/948232], Loss: 0.0056523988\n",
      "Epoch [1/10], Step [389120/948232], Loss: 0.0042825500\n",
      "Epoch [1/10], Step [409600/948232], Loss: 0.0035706945\n",
      "Epoch [1/10], Step [430080/948232], Loss: 0.0030682536\n",
      "Epoch [1/10], Step [450560/948232], Loss: 0.0039911428\n",
      "Epoch [1/10], Step [471040/948232], Loss: 0.0036692398\n",
      "Epoch [1/10], Step [491520/948232], Loss: 0.0024079541\n",
      "Epoch [1/10], Step [512000/948232], Loss: 0.0026777368\n",
      "Epoch [1/10], Step [532480/948232], Loss: 0.0021432438\n",
      "Epoch [1/10], Step [552960/948232], Loss: 0.0018516807\n",
      "Epoch [1/10], Step [573440/948232], Loss: 0.0019381603\n",
      "Epoch [1/10], Step [593920/948232], Loss: 0.0017703031\n",
      "Epoch [1/10], Step [614400/948232], Loss: 0.0019105464\n",
      "Epoch [1/10], Step [634880/948232], Loss: 0.0013697182\n",
      "Epoch [1/10], Step [655360/948232], Loss: 0.0017160793\n",
      "Epoch [1/10], Step [675840/948232], Loss: 0.0017565013\n",
      "Epoch [1/10], Step [696320/948232], Loss: 0.0017965838\n",
      "Epoch [1/10], Step [716800/948232], Loss: 0.0018601655\n",
      "Epoch [1/10], Step [737280/948232], Loss: 0.0015272894\n",
      "Epoch [1/10], Step [757760/948232], Loss: 0.0011362841\n",
      "Epoch [1/10], Step [778240/948232], Loss: 0.0011010779\n",
      "Epoch [1/10], Step [798720/948232], Loss: 0.0014102689\n",
      "Epoch [1/10], Step [819200/948232], Loss: 0.0008554562\n",
      "Epoch [1/10], Step [839680/948232], Loss: 0.0009805954\n",
      "Epoch [1/10], Step [860160/948232], Loss: 0.0012454560\n",
      "Epoch [1/10], Step [880640/948232], Loss: 0.0012994714\n",
      "Epoch [1/10], Step [901120/948232], Loss: 0.0016679256\n",
      "Epoch [1/10], Step [921600/948232], Loss: 0.0019469501\n",
      "Epoch [1/10], Step [942080/948232], Loss: 0.0009312461\n",
      "Epoch [2/10], Step [0/948232], Loss: 0.0010106520\n",
      "Epoch [2/10], Step [20480/948232], Loss: 0.0012496718\n",
      "Epoch [2/10], Step [40960/948232], Loss: 0.0012152444\n",
      "Epoch [2/10], Step [61440/948232], Loss: 0.0006167308\n",
      "Epoch [2/10], Step [81920/948232], Loss: 0.0006683604\n",
      "Epoch [2/10], Step [102400/948232], Loss: 0.0007905728\n",
      "Epoch [2/10], Step [122880/948232], Loss: 0.0006676794\n",
      "Epoch [2/10], Step [143360/948232], Loss: 0.0006701837\n",
      "Epoch [2/10], Step [163840/948232], Loss: 0.0006389436\n",
      "Epoch [2/10], Step [184320/948232], Loss: 0.0006198591\n",
      "Epoch [2/10], Step [204800/948232], Loss: 0.0005650446\n",
      "Epoch [2/10], Step [225280/948232], Loss: 0.0004298459\n",
      "Epoch [2/10], Step [245760/948232], Loss: 0.0004977401\n",
      "Epoch [2/10], Step [266240/948232], Loss: 0.0011026401\n",
      "Epoch [2/10], Step [286720/948232], Loss: 0.0004925070\n",
      "Epoch [2/10], Step [307200/948232], Loss: 0.0005420414\n",
      "Epoch [2/10], Step [327680/948232], Loss: 0.0004344872\n",
      "Epoch [2/10], Step [348160/948232], Loss: 0.0005872684\n",
      "Epoch [2/10], Step [368640/948232], Loss: 0.0004796534\n",
      "Epoch [2/10], Step [389120/948232], Loss: 0.0009595673\n",
      "Epoch [2/10], Step [409600/948232], Loss: 0.0005905936\n",
      "Epoch [2/10], Step [430080/948232], Loss: 0.0003845001\n",
      "Epoch [2/10], Step [450560/948232], Loss: 0.0005782821\n",
      "Epoch [2/10], Step [471040/948232], Loss: 0.0003134770\n",
      "Epoch [2/10], Step [491520/948232], Loss: 0.0003044348\n",
      "Epoch [2/10], Step [512000/948232], Loss: 0.0003474760\n",
      "Epoch [2/10], Step [532480/948232], Loss: 0.0004613057\n",
      "Epoch [2/10], Step [552960/948232], Loss: 0.0003275005\n",
      "Epoch [2/10], Step [573440/948232], Loss: 0.0003713201\n",
      "Epoch [2/10], Step [593920/948232], Loss: 0.0005462825\n",
      "Epoch [2/10], Step [614400/948232], Loss: 0.0004607501\n",
      "Epoch [2/10], Step [634880/948232], Loss: 0.0003608981\n",
      "Epoch [2/10], Step [655360/948232], Loss: 0.0006198938\n",
      "Epoch [2/10], Step [675840/948232], Loss: 0.0004355331\n",
      "Epoch [2/10], Step [696320/948232], Loss: 0.0002895478\n",
      "Epoch [2/10], Step [716800/948232], Loss: 0.0003367834\n",
      "Epoch [2/10], Step [737280/948232], Loss: 0.0006753536\n",
      "Epoch [2/10], Step [757760/948232], Loss: 0.0004995936\n",
      "Epoch [2/10], Step [778240/948232], Loss: 0.0002951532\n",
      "Epoch [2/10], Step [798720/948232], Loss: 0.0003921061\n",
      "Epoch [2/10], Step [819200/948232], Loss: 0.0005496385\n",
      "Epoch [2/10], Step [839680/948232], Loss: 0.0007675001\n",
      "Epoch [2/10], Step [860160/948232], Loss: 0.0006777119\n",
      "Epoch [2/10], Step [880640/948232], Loss: 0.0004556467\n",
      "Epoch [2/10], Step [901120/948232], Loss: 0.0007314865\n",
      "Epoch [2/10], Step [921600/948232], Loss: 0.0002287526\n",
      "Epoch [2/10], Step [942080/948232], Loss: 0.0008676897\n",
      "Epoch [3/10], Step [0/948232], Loss: 0.0003211802\n",
      "Epoch [3/10], Step [20480/948232], Loss: 0.0002270368\n",
      "Epoch [3/10], Step [40960/948232], Loss: 0.0001892437\n",
      "Epoch [3/10], Step [61440/948232], Loss: 0.0003346545\n",
      "Epoch [3/10], Step [81920/948232], Loss: 0.0001776287\n",
      "Epoch [3/10], Step [102400/948232], Loss: 0.0002534288\n",
      "Epoch [3/10], Step [122880/948232], Loss: 0.0003926425\n",
      "Epoch [3/10], Step [143360/948232], Loss: 0.0002705881\n",
      "Epoch [3/10], Step [163840/948232], Loss: 0.0001919684\n",
      "Epoch [3/10], Step [184320/948232], Loss: 0.0002644487\n",
      "Epoch [3/10], Step [204800/948232], Loss: 0.0002485660\n",
      "Epoch [3/10], Step [225280/948232], Loss: 0.0001976606\n",
      "Epoch [3/10], Step [245760/948232], Loss: 0.0001750303\n",
      "Epoch [3/10], Step [266240/948232], Loss: 0.0003891979\n",
      "Epoch [3/10], Step [286720/948232], Loss: 0.0001905163\n",
      "Epoch [3/10], Step [307200/948232], Loss: 0.0012646731\n",
      "Epoch [3/10], Step [327680/948232], Loss: 0.0001820665\n",
      "Epoch [3/10], Step [348160/948232], Loss: 0.0007741365\n",
      "Epoch [3/10], Step [368640/948232], Loss: 0.0008024541\n",
      "Epoch [3/10], Step [389120/948232], Loss: 0.0001719853\n",
      "Epoch [3/10], Step [409600/948232], Loss: 0.0005417854\n",
      "Epoch [3/10], Step [430080/948232], Loss: 0.0001605827\n",
      "Epoch [3/10], Step [450560/948232], Loss: 0.0002387270\n",
      "Epoch [3/10], Step [471040/948232], Loss: 0.0001169102\n",
      "Epoch [3/10], Step [491520/948232], Loss: 0.0005895938\n",
      "Epoch [3/10], Step [512000/948232], Loss: 0.0001555000\n",
      "Epoch [3/10], Step [532480/948232], Loss: 0.0001139779\n",
      "Epoch [3/10], Step [552960/948232], Loss: 0.0001651132\n",
      "Epoch [3/10], Step [573440/948232], Loss: 0.0001851629\n",
      "Epoch [3/10], Step [593920/948232], Loss: 0.0009905855\n",
      "Epoch [3/10], Step [614400/948232], Loss: 0.0001540975\n",
      "Epoch [3/10], Step [634880/948232], Loss: 0.0001056556\n",
      "Epoch [3/10], Step [655360/948232], Loss: 0.0002405605\n",
      "Epoch [3/10], Step [675840/948232], Loss: 0.0001466883\n",
      "Epoch [3/10], Step [696320/948232], Loss: 0.0003007889\n",
      "Epoch [3/10], Step [716800/948232], Loss: 0.0001326917\n",
      "Epoch [3/10], Step [737280/948232], Loss: 0.0003083596\n",
      "Epoch [3/10], Step [757760/948232], Loss: 0.0003733755\n",
      "Epoch [3/10], Step [778240/948232], Loss: 0.0001261313\n",
      "Epoch [3/10], Step [798720/948232], Loss: 0.0004472923\n",
      "Epoch [3/10], Step [819200/948232], Loss: 0.0004265782\n",
      "Epoch [3/10], Step [839680/948232], Loss: 0.0004204733\n",
      "Epoch [3/10], Step [860160/948232], Loss: 0.0004597590\n",
      "Epoch [3/10], Step [880640/948232], Loss: 0.0019984934\n",
      "Epoch [3/10], Step [901120/948232], Loss: 0.0002019910\n",
      "Epoch [3/10], Step [921600/948232], Loss: 0.0002064544\n",
      "Epoch [3/10], Step [942080/948232], Loss: 0.0005248300\n",
      "Epoch [4/10], Step [0/948232], Loss: 0.0016216135\n",
      "Epoch [4/10], Step [20480/948232], Loss: 0.0624034256\n",
      "Epoch [4/10], Step [40960/948232], Loss: 0.0081486721\n",
      "Epoch [4/10], Step [61440/948232], Loss: 0.0079857372\n",
      "Epoch [4/10], Step [81920/948232], Loss: 0.0034675752\n",
      "Epoch [4/10], Step [102400/948232], Loss: 0.0029725011\n",
      "Epoch [4/10], Step [122880/948232], Loss: 0.0013625822\n",
      "Epoch [4/10], Step [143360/948232], Loss: 0.0036036619\n",
      "Epoch [4/10], Step [163840/948232], Loss: 0.0019923048\n",
      "Epoch [4/10], Step [184320/948232], Loss: 0.0010702473\n",
      "Epoch [4/10], Step [204800/948232], Loss: 0.0009741825\n",
      "Epoch [4/10], Step [225280/948232], Loss: 0.0025938936\n",
      "Epoch [4/10], Step [245760/948232], Loss: 0.0015499266\n",
      "Epoch [4/10], Step [266240/948232], Loss: 0.0007037791\n",
      "Epoch [4/10], Step [286720/948232], Loss: 0.0008564586\n",
      "Epoch [4/10], Step [307200/948232], Loss: 0.0005163309\n",
      "Epoch [4/10], Step [327680/948232], Loss: 0.0011057565\n",
      "Epoch [4/10], Step [348160/948232], Loss: 0.0014078021\n",
      "Epoch [4/10], Step [368640/948232], Loss: 0.0007460505\n",
      "Epoch [4/10], Step [389120/948232], Loss: 0.0005933323\n",
      "Epoch [4/10], Step [409600/948232], Loss: 0.0007658858\n",
      "Epoch [4/10], Step [430080/948232], Loss: 0.0003855163\n",
      "Epoch [4/10], Step [450560/948232], Loss: 0.0017029137\n",
      "Epoch [4/10], Step [471040/948232], Loss: 0.0003597514\n",
      "Epoch [4/10], Step [491520/948232], Loss: 0.0002887717\n",
      "Epoch [4/10], Step [512000/948232], Loss: 0.0012750694\n",
      "Epoch [4/10], Step [532480/948232], Loss: 0.0010334037\n",
      "Epoch [4/10], Step [552960/948232], Loss: 0.0007604531\n",
      "Epoch [4/10], Step [573440/948232], Loss: 0.0002962153\n",
      "Epoch [4/10], Step [593920/948232], Loss: 0.0005023463\n",
      "Epoch [4/10], Step [614400/948232], Loss: 0.0009860483\n",
      "Epoch [4/10], Step [634880/948232], Loss: 0.0007877656\n",
      "Epoch [4/10], Step [655360/948232], Loss: 0.0002469052\n",
      "Epoch [4/10], Step [675840/948232], Loss: 0.0005475722\n",
      "Epoch [4/10], Step [696320/948232], Loss: 0.0005590287\n",
      "Epoch [4/10], Step [716800/948232], Loss: 0.0005260104\n",
      "Epoch [4/10], Step [737280/948232], Loss: 0.0004347836\n",
      "Epoch [4/10], Step [757760/948232], Loss: 0.0002998117\n",
      "Epoch [4/10], Step [778240/948232], Loss: 0.0014991427\n",
      "Epoch [4/10], Step [798720/948232], Loss: 0.0005311331\n",
      "Epoch [4/10], Step [819200/948232], Loss: 0.0002232805\n",
      "Epoch [4/10], Step [839680/948232], Loss: 0.0001893983\n",
      "Epoch [4/10], Step [860160/948232], Loss: 0.0002467736\n",
      "Epoch [4/10], Step [880640/948232], Loss: 0.0005964127\n",
      "Epoch [4/10], Step [901120/948232], Loss: 0.0002786180\n",
      "Epoch [4/10], Step [921600/948232], Loss: 0.0001827930\n",
      "Epoch [4/10], Step [942080/948232], Loss: 0.0007693362\n",
      "Epoch [5/10], Step [0/948232], Loss: 0.0006691077\n",
      "Epoch [5/10], Step [20480/948232], Loss: 0.0021269666\n",
      "Epoch [5/10], Step [40960/948232], Loss: 0.0013173963\n",
      "Epoch [5/10], Step [61440/948232], Loss: 0.0018602824\n",
      "Epoch [5/10], Step [81920/948232], Loss: 0.0016988451\n",
      "Epoch [5/10], Step [102400/948232], Loss: 0.0009935510\n",
      "Epoch [5/10], Step [122880/948232], Loss: 0.0003234047\n",
      "Epoch [5/10], Step [143360/948232], Loss: 0.0004982426\n",
      "Epoch [5/10], Step [163840/948232], Loss: 0.0006065300\n",
      "Epoch [5/10], Step [184320/948232], Loss: 0.0004587510\n",
      "Epoch [5/10], Step [204800/948232], Loss: 0.0006526759\n",
      "Epoch [5/10], Step [225280/948232], Loss: 0.0003015566\n",
      "Epoch [5/10], Step [245760/948232], Loss: 0.0004621965\n",
      "Epoch [5/10], Step [266240/948232], Loss: 0.0006597134\n",
      "Epoch [5/10], Step [286720/948232], Loss: 0.0007061677\n",
      "Epoch [5/10], Step [307200/948232], Loss: 0.0002687166\n",
      "Epoch [5/10], Step [327680/948232], Loss: 0.0001644141\n",
      "Epoch [5/10], Step [348160/948232], Loss: 0.0003651782\n",
      "Epoch [5/10], Step [368640/948232], Loss: 0.0002550080\n",
      "Epoch [5/10], Step [389120/948232], Loss: 0.0001553940\n",
      "Epoch [5/10], Step [409600/948232], Loss: 0.0003005787\n",
      "Epoch [5/10], Step [430080/948232], Loss: 0.0001971449\n",
      "Epoch [5/10], Step [450560/948232], Loss: 0.0001223962\n",
      "Epoch [5/10], Step [471040/948232], Loss: 0.0001888282\n",
      "Epoch [5/10], Step [491520/948232], Loss: 0.0003150586\n",
      "Epoch [5/10], Step [512000/948232], Loss: 0.0004282856\n",
      "Epoch [5/10], Step [532480/948232], Loss: 0.0004920001\n",
      "Epoch [5/10], Step [552960/948232], Loss: 0.0001883242\n",
      "Epoch [5/10], Step [573440/948232], Loss: 0.0002247689\n",
      "Epoch [5/10], Step [593920/948232], Loss: 0.0001521601\n",
      "Epoch [5/10], Step [614400/948232], Loss: 0.0002130503\n",
      "Epoch [5/10], Step [634880/948232], Loss: 0.0001341892\n",
      "Epoch [5/10], Step [655360/948232], Loss: 0.0005152165\n",
      "Epoch [5/10], Step [675840/948232], Loss: 0.0001694520\n",
      "Epoch [5/10], Step [696320/948232], Loss: 0.0004536857\n",
      "Epoch [5/10], Step [716800/948232], Loss: 0.0001508243\n",
      "Epoch [5/10], Step [737280/948232], Loss: 0.0003088868\n",
      "Epoch [5/10], Step [757760/948232], Loss: 0.0000969539\n",
      "Epoch [5/10], Step [778240/948232], Loss: 0.0002077655\n",
      "Epoch [5/10], Step [798720/948232], Loss: 0.0001663860\n",
      "Epoch [5/10], Step [819200/948232], Loss: 0.0001342651\n",
      "Epoch [5/10], Step [839680/948232], Loss: 0.0000873840\n",
      "Epoch [5/10], Step [860160/948232], Loss: 0.0000951510\n",
      "Epoch [5/10], Step [880640/948232], Loss: 0.0001311422\n",
      "Epoch [5/10], Step [901120/948232], Loss: 0.0001185504\n",
      "Epoch [5/10], Step [921600/948232], Loss: 0.0005012308\n",
      "Epoch [5/10], Step [942080/948232], Loss: 0.0000986291\n",
      "Epoch [6/10], Step [0/948232], Loss: 0.0010189998\n",
      "Epoch [6/10], Step [20480/948232], Loss: 0.0001086105\n",
      "Epoch [6/10], Step [40960/948232], Loss: 0.0001737343\n",
      "Epoch [6/10], Step [61440/948232], Loss: 0.0001060978\n",
      "Epoch [6/10], Step [81920/948232], Loss: 0.0002671420\n",
      "Epoch [6/10], Step [102400/948232], Loss: 0.0001339689\n",
      "Epoch [6/10], Step [122880/948232], Loss: 0.0000841497\n",
      "Epoch [6/10], Step [143360/948232], Loss: 0.0016216550\n",
      "Epoch [6/10], Step [163840/948232], Loss: 0.0001678420\n",
      "Epoch [6/10], Step [184320/948232], Loss: 0.0000846397\n",
      "Epoch [6/10], Step [204800/948232], Loss: 0.0002060992\n",
      "Epoch [6/10], Step [225280/948232], Loss: 0.0001335115\n",
      "Epoch [6/10], Step [245760/948232], Loss: 0.0000815614\n",
      "Epoch [6/10], Step [266240/948232], Loss: 0.0002373868\n",
      "Epoch [6/10], Step [286720/948232], Loss: 0.0000599339\n",
      "Epoch [6/10], Step [307200/948232], Loss: 0.0000884410\n",
      "Epoch [6/10], Step [327680/948232], Loss: 0.0002296760\n",
      "Epoch [6/10], Step [348160/948232], Loss: 0.0004615767\n",
      "Epoch [6/10], Step [368640/948232], Loss: 0.0001416929\n",
      "Epoch [6/10], Step [389120/948232], Loss: 0.0001572403\n",
      "Epoch [6/10], Step [409600/948232], Loss: 0.0000758986\n",
      "Epoch [6/10], Step [430080/948232], Loss: 0.0000875732\n",
      "Epoch [6/10], Step [450560/948232], Loss: 0.0001183634\n",
      "Epoch [6/10], Step [471040/948232], Loss: 0.0001149095\n",
      "Epoch [6/10], Step [491520/948232], Loss: 0.0000980672\n",
      "Epoch [6/10], Step [512000/948232], Loss: 0.0000865751\n",
      "Epoch [6/10], Step [532480/948232], Loss: 0.0002749422\n",
      "Epoch [6/10], Step [552960/948232], Loss: 0.0001629097\n",
      "Epoch [6/10], Step [573440/948232], Loss: 0.0001509246\n",
      "Epoch [6/10], Step [593920/948232], Loss: 0.0000907796\n",
      "Epoch [6/10], Step [614400/948232], Loss: 0.0000596812\n",
      "Epoch [6/10], Step [634880/948232], Loss: 0.0007674215\n",
      "Epoch [6/10], Step [655360/948232], Loss: 0.0000564563\n",
      "Epoch [6/10], Step [675840/948232], Loss: 0.0002873256\n",
      "Epoch [6/10], Step [696320/948232], Loss: 0.0000800723\n",
      "Epoch [6/10], Step [716800/948232], Loss: 0.0000512679\n",
      "Epoch [6/10], Step [737280/948232], Loss: 0.0000672437\n",
      "Epoch [6/10], Step [757760/948232], Loss: 0.0000450336\n",
      "Epoch [6/10], Step [778240/948232], Loss: 0.0000531894\n",
      "Epoch [6/10], Step [798720/948232], Loss: 0.0001362941\n",
      "Epoch [6/10], Step [819200/948232], Loss: 0.0000918218\n",
      "Epoch [6/10], Step [839680/948232], Loss: 0.0012117715\n",
      "Epoch [6/10], Step [860160/948232], Loss: 0.0000484588\n",
      "Epoch [6/10], Step [880640/948232], Loss: 0.0000800794\n",
      "Epoch [6/10], Step [901120/948232], Loss: 0.0001148449\n",
      "Epoch [6/10], Step [921600/948232], Loss: 0.0000665109\n",
      "Epoch [6/10], Step [942080/948232], Loss: 0.0000692774\n",
      "Epoch [7/10], Step [0/948232], Loss: 0.0001328953\n",
      "Epoch [7/10], Step [20480/948232], Loss: 0.0000873276\n",
      "Epoch [7/10], Step [40960/948232], Loss: 0.0000641749\n",
      "Epoch [7/10], Step [61440/948232], Loss: 0.0000538128\n",
      "Epoch [7/10], Step [81920/948232], Loss: 0.0000418852\n",
      "Epoch [7/10], Step [102400/948232], Loss: 0.0000416027\n",
      "Epoch [7/10], Step [122880/948232], Loss: 0.0000616879\n",
      "Epoch [7/10], Step [143360/948232], Loss: 0.0000694810\n",
      "Epoch [7/10], Step [163840/948232], Loss: 0.0000601770\n",
      "Epoch [7/10], Step [184320/948232], Loss: 0.0000611505\n",
      "Epoch [7/10], Step [204800/948232], Loss: 0.0002422044\n",
      "Epoch [7/10], Step [225280/948232], Loss: 0.0001354515\n",
      "Epoch [7/10], Step [245760/948232], Loss: 0.0000433872\n",
      "Epoch [7/10], Step [266240/948232], Loss: 0.0000758327\n",
      "Epoch [7/10], Step [286720/948232], Loss: 0.0001096868\n",
      "Epoch [7/10], Step [307200/948232], Loss: 0.0001362204\n",
      "Epoch [7/10], Step [327680/948232], Loss: 0.0001543957\n",
      "Epoch [7/10], Step [348160/948232], Loss: 0.0000781144\n",
      "Epoch [7/10], Step [368640/948232], Loss: 0.0000462266\n",
      "Epoch [7/10], Step [389120/948232], Loss: 0.0001002005\n",
      "Epoch [7/10], Step [409600/948232], Loss: 0.0000496729\n",
      "Epoch [7/10], Step [430080/948232], Loss: 0.0000674168\n",
      "Epoch [7/10], Step [450560/948232], Loss: 0.0000434949\n",
      "Epoch [7/10], Step [471040/948232], Loss: 0.0000404527\n",
      "Epoch [7/10], Step [491520/948232], Loss: 0.0001928388\n",
      "Epoch [7/10], Step [512000/948232], Loss: 0.0000481232\n",
      "Epoch [7/10], Step [532480/948232], Loss: 0.0000731773\n",
      "Epoch [7/10], Step [552960/948232], Loss: 0.0000643629\n",
      "Epoch [7/10], Step [573440/948232], Loss: 0.0000383388\n",
      "Epoch [7/10], Step [593920/948232], Loss: 0.0000623337\n",
      "Epoch [7/10], Step [614400/948232], Loss: 0.0000431214\n",
      "Epoch [7/10], Step [634880/948232], Loss: 0.0000384078\n",
      "Epoch [7/10], Step [655360/948232], Loss: 0.0000667754\n",
      "Epoch [7/10], Step [675840/948232], Loss: 0.0000353618\n",
      "Epoch [7/10], Step [696320/948232], Loss: 0.0000608631\n",
      "Epoch [7/10], Step [716800/948232], Loss: 0.0000554899\n",
      "Epoch [7/10], Step [737280/948232], Loss: 0.0000279437\n",
      "Epoch [7/10], Step [757760/948232], Loss: 0.0000630829\n",
      "Epoch [7/10], Step [778240/948232], Loss: 0.0000471770\n",
      "Epoch [7/10], Step [798720/948232], Loss: 0.0000476041\n",
      "Epoch [7/10], Step [819200/948232], Loss: 0.0000398271\n",
      "Epoch [7/10], Step [839680/948232], Loss: 0.0002264461\n",
      "Epoch [7/10], Step [860160/948232], Loss: 0.0000526437\n",
      "Epoch [7/10], Step [880640/948232], Loss: 0.0000400795\n",
      "Epoch [7/10], Step [901120/948232], Loss: 0.0000402200\n",
      "Epoch [7/10], Step [921600/948232], Loss: 0.0047856821\n",
      "Epoch [7/10], Step [942080/948232], Loss: 0.0000621339\n",
      "Epoch [8/10], Step [0/948232], Loss: 0.0000453523\n",
      "Epoch [8/10], Step [20480/948232], Loss: 0.0000330391\n",
      "Epoch [8/10], Step [40960/948232], Loss: 0.0000524664\n",
      "Epoch [8/10], Step [61440/948232], Loss: 0.0000427176\n",
      "Epoch [8/10], Step [81920/948232], Loss: 0.0000624037\n",
      "Epoch [8/10], Step [102400/948232], Loss: 0.0000309122\n",
      "Epoch [8/10], Step [122880/948232], Loss: 0.0000470413\n",
      "Epoch [8/10], Step [143360/948232], Loss: 0.0001251917\n",
      "Epoch [8/10], Step [163840/948232], Loss: 0.0000949451\n",
      "Epoch [8/10], Step [184320/948232], Loss: 0.0000355293\n",
      "Epoch [8/10], Step [204800/948232], Loss: 0.0000348439\n",
      "Epoch [8/10], Step [225280/948232], Loss: 0.0000336564\n",
      "Epoch [8/10], Step [245760/948232], Loss: 0.0000543725\n",
      "Epoch [8/10], Step [266240/948232], Loss: 0.0000469488\n",
      "Epoch [8/10], Step [286720/948232], Loss: 0.0001279688\n",
      "Epoch [8/10], Step [307200/948232], Loss: 0.0000430217\n",
      "Epoch [8/10], Step [327680/948232], Loss: 0.0000375937\n",
      "Epoch [8/10], Step [348160/948232], Loss: 0.0000484441\n",
      "Epoch [8/10], Step [368640/948232], Loss: 0.0000514513\n",
      "Epoch [8/10], Step [389120/948232], Loss: 0.0000412709\n",
      "Epoch [8/10], Step [409600/948232], Loss: 0.0000403115\n",
      "Epoch [8/10], Step [430080/948232], Loss: 0.0018953180\n",
      "Epoch [8/10], Step [450560/948232], Loss: 0.0001191438\n",
      "Epoch [8/10], Step [471040/948232], Loss: 0.0000772416\n",
      "Epoch [8/10], Step [491520/948232], Loss: 0.0000393378\n",
      "Epoch [8/10], Step [512000/948232], Loss: 0.0000370112\n",
      "Epoch [8/10], Step [532480/948232], Loss: 0.0000442864\n",
      "Epoch [8/10], Step [552960/948232], Loss: 0.0000410775\n",
      "Epoch [8/10], Step [573440/948232], Loss: 0.0000348318\n",
      "Epoch [8/10], Step [593920/948232], Loss: 0.0000611779\n",
      "Epoch [8/10], Step [614400/948232], Loss: 0.0000844340\n",
      "Epoch [8/10], Step [634880/948232], Loss: 0.0000406042\n",
      "Epoch [8/10], Step [655360/948232], Loss: 0.0000327429\n",
      "Epoch [8/10], Step [675840/948232], Loss: 0.0003864952\n",
      "Epoch [8/10], Step [696320/948232], Loss: 0.0000234942\n",
      "Epoch [8/10], Step [716800/948232], Loss: 0.0000338265\n",
      "Epoch [8/10], Step [737280/948232], Loss: 0.0000309325\n",
      "Epoch [8/10], Step [757760/948232], Loss: 0.0000309362\n",
      "Epoch [8/10], Step [778240/948232], Loss: 0.0000257084\n",
      "Epoch [8/10], Step [798720/948232], Loss: 0.0000321462\n",
      "Epoch [8/10], Step [819200/948232], Loss: 0.0000265214\n",
      "Epoch [8/10], Step [839680/948232], Loss: 0.0000250131\n",
      "Epoch [8/10], Step [860160/948232], Loss: 0.0000350635\n",
      "Epoch [8/10], Step [880640/948232], Loss: 0.0000264260\n",
      "Epoch [8/10], Step [901120/948232], Loss: 0.0000270597\n",
      "Epoch [8/10], Step [921600/948232], Loss: 0.0000443963\n",
      "Epoch [8/10], Step [942080/948232], Loss: 0.0000387973\n",
      "Epoch [9/10], Step [0/948232], Loss: 0.0000230711\n",
      "Epoch [9/10], Step [20480/948232], Loss: 0.0062150476\n",
      "Epoch [9/10], Step [40960/948232], Loss: 0.0000324138\n",
      "Epoch [9/10], Step [61440/948232], Loss: 0.0000266012\n",
      "Epoch [9/10], Step [81920/948232], Loss: 0.0000282103\n",
      "Epoch [9/10], Step [102400/948232], Loss: 0.0000616313\n",
      "Epoch [9/10], Step [122880/948232], Loss: 0.0000228462\n",
      "Epoch [9/10], Step [143360/948232], Loss: 0.0000288088\n",
      "Epoch [9/10], Step [163840/948232], Loss: 0.0011052340\n",
      "Epoch [9/10], Step [184320/948232], Loss: 0.0000265540\n",
      "Epoch [9/10], Step [204800/948232], Loss: 0.0000351165\n",
      "Epoch [9/10], Step [225280/948232], Loss: 0.0000747737\n",
      "Epoch [9/10], Step [245760/948232], Loss: 0.0000371992\n",
      "Epoch [9/10], Step [266240/948232], Loss: 0.0001103271\n",
      "Epoch [9/10], Step [286720/948232], Loss: 0.0002433614\n",
      "Epoch [9/10], Step [307200/948232], Loss: 0.0001263681\n",
      "Epoch [9/10], Step [327680/948232], Loss: 0.0000725076\n",
      "Epoch [9/10], Step [348160/948232], Loss: 0.0000359692\n",
      "Epoch [9/10], Step [368640/948232], Loss: 0.0003021117\n",
      "Epoch [9/10], Step [389120/948232], Loss: 0.0000347710\n",
      "Epoch [9/10], Step [409600/948232], Loss: 0.0000352779\n",
      "Epoch [9/10], Step [430080/948232], Loss: 0.0000360675\n",
      "Epoch [9/10], Step [450560/948232], Loss: 0.0023091899\n",
      "Epoch [9/10], Step [471040/948232], Loss: 0.0000568263\n",
      "Epoch [9/10], Step [491520/948232], Loss: 0.0000439775\n",
      "Epoch [9/10], Step [512000/948232], Loss: 0.0000261992\n",
      "Epoch [9/10], Step [532480/948232], Loss: 0.0000163484\n",
      "Epoch [9/10], Step [552960/948232], Loss: 0.0000256649\n",
      "Epoch [9/10], Step [573440/948232], Loss: 0.0000202483\n",
      "Epoch [9/10], Step [593920/948232], Loss: 0.0005381678\n",
      "Epoch [9/10], Step [614400/948232], Loss: 0.0000239295\n",
      "Epoch [9/10], Step [634880/948232], Loss: 0.0000553551\n",
      "Epoch [9/10], Step [655360/948232], Loss: 0.0000249656\n",
      "Epoch [9/10], Step [675840/948232], Loss: 0.0000297270\n",
      "Epoch [9/10], Step [696320/948232], Loss: 0.0007884035\n",
      "Epoch [9/10], Step [716800/948232], Loss: 0.0000763595\n",
      "Epoch [9/10], Step [737280/948232], Loss: 0.0000781297\n",
      "Epoch [9/10], Step [757760/948232], Loss: 0.0000224939\n",
      "Epoch [9/10], Step [778240/948232], Loss: 0.0000319516\n",
      "Epoch [9/10], Step [798720/948232], Loss: 0.0000520247\n",
      "Epoch [9/10], Step [819200/948232], Loss: 0.0000280032\n",
      "Epoch [9/10], Step [839680/948232], Loss: 0.0000643691\n",
      "Epoch [9/10], Step [860160/948232], Loss: 0.0000689020\n",
      "Epoch [9/10], Step [880640/948232], Loss: 0.0000433750\n",
      "Epoch [9/10], Step [901120/948232], Loss: 0.0000252259\n",
      "Epoch [9/10], Step [921600/948232], Loss: 0.0005987404\n",
      "Epoch [9/10], Step [942080/948232], Loss: 0.0000418788\n",
      "Epoch [10/10], Step [0/948232], Loss: 0.0007262441\n",
      "Epoch [10/10], Step [20480/948232], Loss: 0.0519303717\n",
      "Epoch [10/10], Step [40960/948232], Loss: 0.0086840643\n",
      "Epoch [10/10], Step [61440/948232], Loss: 0.0083198473\n",
      "Epoch [10/10], Step [81920/948232], Loss: 0.0029105805\n",
      "Epoch [10/10], Step [102400/948232], Loss: 0.0012005609\n",
      "Epoch [10/10], Step [122880/948232], Loss: 0.0031559723\n",
      "Epoch [10/10], Step [143360/948232], Loss: 0.0012772717\n",
      "Epoch [10/10], Step [163840/948232], Loss: 0.0018700273\n",
      "Epoch [10/10], Step [184320/948232], Loss: 0.0011503763\n",
      "Epoch [10/10], Step [204800/948232], Loss: 0.0007467124\n",
      "Epoch [10/10], Step [225280/948232], Loss: 0.0007290670\n",
      "Epoch [10/10], Step [245760/948232], Loss: 0.0007138372\n",
      "Epoch [10/10], Step [266240/948232], Loss: 0.0005179674\n",
      "Epoch [10/10], Step [286720/948232], Loss: 0.0004191497\n",
      "Epoch [10/10], Step [307200/948232], Loss: 0.0010553885\n",
      "Epoch [10/10], Step [327680/948232], Loss: 0.0015623901\n",
      "Epoch [10/10], Step [348160/948232], Loss: 0.0006262154\n",
      "Epoch [10/10], Step [368640/948232], Loss: 0.0003712974\n",
      "Epoch [10/10], Step [389120/948232], Loss: 0.0004165528\n",
      "Epoch [10/10], Step [409600/948232], Loss: 0.0004897121\n",
      "Epoch [10/10], Step [430080/948232], Loss: 0.0008541249\n",
      "Epoch [10/10], Step [450560/948232], Loss: 0.0020668383\n",
      "Epoch [10/10], Step [471040/948232], Loss: 0.0003873214\n",
      "Epoch [10/10], Step [491520/948232], Loss: 0.0007527347\n",
      "Epoch [10/10], Step [512000/948232], Loss: 0.0012214892\n",
      "Epoch [10/10], Step [532480/948232], Loss: 0.0002718370\n",
      "Epoch [10/10], Step [552960/948232], Loss: 0.0001747686\n",
      "Epoch [10/10], Step [573440/948232], Loss: 0.0005265668\n",
      "Epoch [10/10], Step [593920/948232], Loss: 0.0002800247\n",
      "Epoch [10/10], Step [614400/948232], Loss: 0.0005412139\n",
      "Epoch [10/10], Step [634880/948232], Loss: 0.0007411367\n",
      "Epoch [10/10], Step [655360/948232], Loss: 0.0003540119\n",
      "Epoch [10/10], Step [675840/948232], Loss: 0.0008844503\n",
      "Epoch [10/10], Step [696320/948232], Loss: 0.0003009635\n",
      "Epoch [10/10], Step [716800/948232], Loss: 0.0003199069\n",
      "Epoch [10/10], Step [737280/948232], Loss: 0.0004631556\n",
      "Epoch [10/10], Step [757760/948232], Loss: 0.0004146974\n",
      "Epoch [10/10], Step [778240/948232], Loss: 0.0003638762\n",
      "Epoch [10/10], Step [798720/948232], Loss: 0.0002218034\n",
      "Epoch [10/10], Step [819200/948232], Loss: 0.0001968480\n",
      "Epoch [10/10], Step [839680/948232], Loss: 0.0011278891\n",
      "Epoch [10/10], Step [860160/948232], Loss: 0.0003181415\n",
      "Epoch [10/10], Step [880640/948232], Loss: 0.0002825738\n",
      "Epoch [10/10], Step [901120/948232], Loss: 0.0001637951\n",
      "Epoch [10/10], Step [921600/948232], Loss: 0.0001764664\n",
      "Epoch [10/10], Step [942080/948232], Loss: 0.0001531674\n"
     ]
    }
   ],
   "source": [
    "# train mlp\n",
    "num_epochs = 10\n",
    "mlp_clf.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data1, data2, labels) in enumerate(train_dataloader):\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = mlp_clf(data1,data2)\n",
    "        loss = criterion(outputs , labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.10f}'\n",
    "                  .format(epoch+1, num_epochs, i*len(data1), len(train_dataloader.dataset),loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:03:32.396851300Z",
     "start_time": "2023-06-20T09:56:22.332378900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class NodeTestset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data1 = self.data.iloc[index]['id1']\n",
    "        data2 = self.data.iloc[index]['id2']\n",
    "        return data1,data2\n",
    "test_dataloader = DataLoader(NodeTestset(test_dataset), batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:03:32.414444400Z",
     "start_time": "2023-06-20T10:03:32.396851300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#mlp pred\n",
    "mlp_clf.eval()\n",
    "y_pred_list = []\n",
    "with torch.no_grad():\n",
    "    for i, (data1, data2) in enumerate(test_dataloader):\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        output = mlp_clf(data1, data2)\n",
    "        output = torch.round(output)\n",
    "        y_pred_list += output.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:03:38.953609700Z",
     "start_time": "2023-06-20T10:03:32.413444600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='auc', feature_types=None,\n",
      "              gamma=0, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.3, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
      "              min_child_weight=13, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, nthread=8, num_parallel_tree=None,\n",
      "              predictor=None, ...)\n",
      "train RandomForestClassifier(max_depth=50, min_samples_split=10, n_estimators=200,\n",
      "                       n_jobs=8)\n"
     ]
    }
   ],
   "source": [
    "#train xgb and rf\n",
    "for idx, classifier in enumerate(classifiers):\n",
    "        print(f'train {str(classifier)}')\n",
    "        classifier.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:04:57.935209500Z",
     "start_time": "2023-06-20T10:03:38.954612600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "all_val_preds = []\n",
    "weights = [0.4, 0.2, 0.4]\n",
    "for idx, classifier in enumerate(classifiers):\n",
    "    y_val_pred = classifier.predict(test_X)\n",
    "    all_val_preds.append(y_val_pred)\n",
    "all_val_preds.append(y_pred_list)\n",
    "#stack preds\n",
    "stacked_val_preds = np.column_stack(all_val_preds)\n",
    "weighted_votes = np.zeros_like(stacked_val_preds, dtype=float)\n",
    "for col_idx, weight in enumerate(weights):\n",
    "    weighted_votes[:, col_idx] = stacked_val_preds[:, col_idx] * weight\n",
    "weighted_sum = np.sum(weighted_votes, axis=1) / np.sum(weights)  # Devide by sum of weights as we are not using already normalized weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:04:59.088833300Z",
     "start_time": "2023-06-20T10:04:57.937209500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "test_weighted_vote = np.where(weighted_sum > threshold, 1, 0)\n",
    "# finally pred\n",
    "test_data['label'] = test_weighted_vote.astype(int)\n",
    "test_final = test_data.drop(columns=['id1','id2'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:04:59.104825300Z",
     "start_time": "2023-06-20T10:04:59.089833500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#save pred\n",
    "test_final.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:04:59.246465400Z",
     "start_time": "2023-06-20T10:04:59.105825Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:04:59.261876300Z",
     "start_time": "2023-06-20T10:04:59.246465400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

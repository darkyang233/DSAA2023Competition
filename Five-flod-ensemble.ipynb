{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:05:13.806719700Z",
     "start_time": "2023-06-20T10:05:13.797216300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import  f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#Load data\n",
    "train_data = pd.read_csv('train.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:05:13.948817300Z",
     "start_time": "2023-06-20T10:05:13.807719300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#Extract features and labels\n",
    "X = train_data.drop(columns=['id', 'label'])\n",
    "y = train_data['label']\n",
    "training_dataset = train_data.drop(['id'],axis=1,inplace=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:05:13.964817800Z",
     "start_time": "2023-06-20T10:05:13.949817400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#Classifier List\n",
    "xgb_clf = XGBClassifier(\n",
    "    learning_rate=0.3,\n",
    "    n_estimators=200,\n",
    "    max_depth=7,\n",
    "    min_child_weight=13,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    nthread=8,\n",
    "    scale_pos_weight=1,\n",
    "    early_stopping_rounds=30,\n",
    "    eval_metric='auc')\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=50,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=1,\n",
    "    n_jobs=8)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:05:13.981817500Z",
     "start_time": "2023-06-20T10:05:13.964817800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# MLP\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(MLPClassifier, self).__init__()\n",
    "            self.input = nn.Sequential(\n",
    "                nn.Embedding(input_size, hidden_size),\n",
    "            )\n",
    "            self.hidden = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "            self.output = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, output_size),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        data1 = self.hidden(self.input(data1))\n",
    "        data2 = self.hidden(self.input(data2))\n",
    "        data = torch.cat((data1, data2), dim=1)\n",
    "        label = self.output(data).squeeze()\n",
    "        return label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:05:13.996817500Z",
     "start_time": "2023-06-20T10:05:13.982817700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#pytorch data\n",
    "class NodeDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data1 = self.data.iloc[index]['id1']\n",
    "        data2 = self.data.iloc[index]['id2']\n",
    "        label = self.data.iloc[index]['label']\n",
    "        return data1,data2,label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:05:14.013816Z",
     "start_time": "2023-06-20T10:05:13.996817500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Create a data loader\n",
    "batch_size = 2048\n",
    "train_dataloader = DataLoader(NodeDataset(training_dataset), batch_size=batch_size, shuffle=True)\n",
    "#Set network parameters\n",
    "input_size = training_dataset.shape[0]\n",
    "hidden_size = 128\n",
    "output_size = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T10:05:14.027815600Z",
     "start_time": "2023-06-20T10:05:14.012816200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Five flod cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "f1_scores = []\n",
    "xgb_f1 = []\n",
    "rf_f1 = []\n",
    "mlp_f1 = []\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(X, y)):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    y_valid_preds = []\n",
    "    print(f'Fold {fold+1}')\n",
    "    #xgb\n",
    "    xgb_clf.fit(X_train, y_train,eval_set=[(X_valid,y_valid)])\n",
    "    #xgb-pred\n",
    "    y_valid_pred = xgb_clf.predict(X_valid)\n",
    "    y_valid_preds.append(y_valid_pred)\n",
    "    #f1-score\n",
    "    f1 = f1_score(y_valid, y_valid_pred)\n",
    "    xgb_f1.append(f1)\n",
    "    print(f\"fold:[{fold+1}/5],xgb-f1={f1}\\n\")\n",
    "    #rf\n",
    "    rf.fit(X_train,y_train)\n",
    "    #rf-pred\n",
    "    y_valid_pred = rf.predict(X_valid)\n",
    "    y_valid_preds.append(y_valid_pred)\n",
    "    #f1-score\n",
    "    f1 = f1_score(y_valid, y_valid_pred)\n",
    "    rf_f1.append(f1)\n",
    "    print(f\"fold:[{fold+1}/5],rf-f1={f1}\\n\")\n",
    "    # mlp\n",
    "    mlp_clf = MLPClassifier(input_size, hidden_size, output_size).to(device)\n",
    "    # loss and adam\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(mlp_clf.parameters(), lr=0.001)\n",
    "    #Transforming an array into a two-dimensional array\n",
    "    y_trains = pd.DataFrame(y_train.to_numpy().reshape(-1,1),columns = ['label'])\n",
    "    y_valids = pd.DataFrame(y_valid.to_numpy().reshape(-1,1),columns = ['label'])\n",
    "    #Divide training and validation sets\n",
    "    train_data = np.concatenate((X_train,y_trains), axis=1)\n",
    "    train_dataset = pd.DataFrame(train_data,columns=['id1','id2','label'])\n",
    "    valid_data = np.concatenate((X_valid,y_valids), axis=1)\n",
    "    valid_dataset = pd.DataFrame(valid_data,columns=['id1','id2','label'])\n",
    "    #Create a data loader\n",
    "    train_dataloader = DataLoader(NodeDataset(train_dataset), batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader(NodeDataset(valid_dataset), batch_size=batch_size, shuffle=False)\n",
    "    #train mlp\n",
    "    num_epochs = 10\n",
    "    mlp_clf.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data1, data2, labels) in enumerate(train_dataloader):\n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = mlp_clf(data1,data2)\n",
    "            loss = criterion(outputs , labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 10 == 0:\n",
    "                print('fold [{}/{}],Epoch [{}/{}], Step [{}/{}], Loss: {:.10f}'\n",
    "                      .format(fold+1, 5, epoch+1, num_epochs, i*len(data1), len(train_dataloader.dataset), loss.item()))\n",
    "    #eval mlp\n",
    "    mlp_clf.eval()\n",
    "    y_pred_list = []\n",
    "    y_true_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data1, data2, labels) in enumerate(valid_dataloader):\n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = mlp_clf(data1, data2)\n",
    "            output = torch.round(output)\n",
    "            y_pred_list += output.tolist()\n",
    "            y_true_list.extend(labels.cpu().numpy())\n",
    "    # f1-score\n",
    "    f1 = f1_score(y_true_list, y_pred_list)\n",
    "    mlp_f1.append(f1)\n",
    "    print(f\"fold:[{fold+1}/5],mlp-f1={f1}\\n\")\n",
    "    y_valid_preds.append(y_pred_list)\n",
    "    weights = [0.4, 0.2, 0.4]\n",
    "    #stack preds\n",
    "    stacked_val_preds = np.column_stack(y_valid_preds)\n",
    "    weighted_votes = np.zeros_like(stacked_val_preds, dtype=float)\n",
    "    for col_idx, weight in enumerate(weights):\n",
    "        weighted_votes[:, col_idx] = stacked_val_preds[:, col_idx] * weight\n",
    "    weighted_sum = np.sum(weighted_votes, axis=1) / np.sum(weights)\n",
    "    threshold = 0.5\n",
    "    y_valid_pred = np.where(weighted_sum > threshold, 1, 0)\n",
    "     #f1-score\n",
    "    f1 = f1_score(y_valid, y_valid_pred)\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"Accuracy scores in {fold+1}-fold cross validation:{f1}\")\n",
    "#print f1-score\n",
    "print(\"xgb accuracy scores in 5-fold cross validation:\", xgb_f1)\n",
    "print(\"Mean accuracy in 5-fold cross validation:\", np.mean(xgb_f1))\n",
    "print(\"rf accuracy scores in 5-fold cross validation:\", rf_f1)\n",
    "print(\"Mean accuracy in 5-fold cross validation:\", np.mean(rf_f1))\n",
    "print(\"mlp accuracy scores in 5-fold cross validation:\", mlp_f1)\n",
    "print(\"Mean accuracy in 5-fold cross validation:\", np.mean(mlp_f1))\n",
    "print(\"Accuracy scores in 5-fold cross validation:\", f1_scores)\n",
    "print(\"Mean accuracy in 5-fold cross validation:\", np.mean(f1_scores))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
